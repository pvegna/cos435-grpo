{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "execution": {
     "iopub.execute_input": "2025-04-29T20:31:07.284143Z",
     "iopub.status.busy": "2025-04-29T20:31:07.283272Z",
     "iopub.status.idle": "2025-04-29T20:31:16.371429Z",
     "shell.execute_reply": "2025-04-29T20:31:16.370422Z",
     "shell.execute_reply.started": "2025-04-29T20:31:07.284111Z"
    },
    "id": "2jr_Q2jsvMDb",
    "outputId": "d703168d-1c65-4832-9a6d-a38a4d63b791",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install numpy==1.23.5 #need to downgrade otherwise bool8 error again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-29T20:31:16.372694Z",
     "iopub.status.busy": "2025-04-29T20:31:16.372441Z",
     "iopub.status.idle": "2025-04-29T20:31:20.531225Z",
     "shell.execute_reply": "2025-04-29T20:31:20.530095Z",
     "shell.execute_reply.started": "2025-04-29T20:31:16.372670Z"
    },
    "id": "Cv8728QoMDQa",
    "outputId": "4190887d-d47a-44d7-9439-aeba07b3dc41",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install box2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-29T20:31:20.534613Z",
     "iopub.status.busy": "2025-04-29T20:31:20.534228Z",
     "iopub.status.idle": "2025-04-29T20:31:41.331560Z",
     "shell.execute_reply": "2025-04-29T20:31:41.330616Z",
     "shell.execute_reply.started": "2025-04-29T20:31:20.534587Z"
    },
    "id": "ZdFVZmMcN6gi",
    "outputId": "8c187e3c-ac95-44d5-9d18-19f96a224710",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install \"gym[atari,accept-rom-license]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T20:31:41.332922Z",
     "iopub.status.busy": "2025-04-29T20:31:41.332600Z",
     "iopub.status.idle": "2025-04-29T20:31:46.198261Z",
     "shell.execute_reply": "2025-04-29T20:31:46.197415Z",
     "shell.execute_reply.started": "2025-04-29T20:31:41.332889Z"
    },
    "id": "_e1tFAQQpvJ5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, act_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        logits = self.forward(obs)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        logp = dist.log_prob(action)\n",
    "        return action, logp, logits\n",
    "\n",
    "    def get_log_probs(self, obs, action):\n",
    "        logits = self.forward(obs)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        return dist.log_prob(action)\n",
    "\n",
    "class Value(nn.Module):\n",
    "    def __init__(self, obs_dim, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "\n",
    "#UTILS\n",
    "def compute_gae(rewards, values, masks, gamma=0.99, lam=0.95):\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    values = values + [0]\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * lam * masks[step] * gae\n",
    "        advantages.insert(0, gae)\n",
    "    return advantages\n",
    "\n",
    "\n",
    "#CNN\n",
    "class CNNPolicy(nn.Module):\n",
    "    def __init__(self, act_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, 8, stride=4),  # (4, 84, 84) -> (32, 20, 20)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2),  # -> (64, 9, 9)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=1),  # -> (64, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, act_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x / 255.0)  # normalize if you not normalized earlier\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        logits = self.forward(obs)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action, dist.log_prob(action), logits\n",
    "\n",
    "    def get_log_probs(self, obs, action):\n",
    "        logits = self.forward(obs)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        return dist.log_prob(action)\n",
    "\n",
    "class CNNValue(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, 8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x / 255.0).squeeze(-1)\n",
    "\n",
    "import cv2\n",
    "#Pong util\n",
    "def preprocess_frame(frame):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    frame = frame[34:194]\n",
    "    frame = cv2.resize(frame, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "    frame = frame / 255.0\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T20:31:46.199871Z",
     "iopub.status.busy": "2025-04-29T20:31:46.199508Z",
     "iopub.status.idle": "2025-04-29T20:31:46.220394Z",
     "shell.execute_reply": "2025-04-29T20:31:46.219440Z",
     "shell.execute_reply.started": "2025-04-29T20:31:46.199826Z"
    },
    "id": "D01SMm4gp84G",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collect_batch(env, policy, value_fn, batch_size):\n",
    "    obs_list, act_list, old_logp_list, logits_list = [], [], [], []\n",
    "    rew_list, val_list, mask_list = [], [], []\n",
    "    episode_returns = []\n",
    "\n",
    "    result = env.reset()\n",
    "    if isinstance(result, tuple):\n",
    "        obs, _ = result\n",
    "    else:\n",
    "        obs = result\n",
    "\n",
    "    done = False\n",
    "    steps = 0\n",
    "    episode_reward = 0\n",
    "\n",
    "    while steps < batch_size:\n",
    "        # obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
    "\n",
    "        # logits = policy(obs_tensor)\n",
    "        # probs = torch.softmax(logits, dim=-1)\n",
    "        # dist = torch.distributions.Categorical(probs)\n",
    "        # action = dist.sample()\n",
    "        # logp = dist.log_prob(action)\n",
    "\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs_array = obs\n",
    "        else:\n",
    "            obs_array = np.array(obs, copy=False)  # <-- fix for LazyFrames\n",
    "\n",
    "        obs_tensor = torch.from_numpy(obs_array).float().unsqueeze(0)\n",
    "\n",
    "        action, logp, logits = policy.get_action(obs_tensor)\n",
    "        value = value_fn(obs_tensor)\n",
    "\n",
    "        result = env.step(action.item())\n",
    "        if len(result) == 5:\n",
    "            next_obs, reward, terminated, truncated, _ = result\n",
    "            done = terminated or truncated\n",
    "        else:\n",
    "            next_obs, reward, done, _ = result\n",
    "\n",
    "        obs_list.append(obs_tensor.squeeze(0))\n",
    "        act_list.append(action)\n",
    "        old_logp_list.append(logp.detach())\n",
    "        logits_list.append(logits.detach())\n",
    "        rew_list.append(reward)\n",
    "        val_list.append(value)\n",
    "        mask_list.append(1 - int(done))\n",
    "\n",
    "        episode_reward += reward\n",
    "        obs = next_obs\n",
    "        steps += 1\n",
    "\n",
    "        if done:\n",
    "            episode_returns.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "            result = env.reset()\n",
    "            if isinstance(result, tuple):\n",
    "                obs, _ = result\n",
    "            else:\n",
    "                obs = result\n",
    "\n",
    "    return obs_list, act_list, old_logp_list, logits_list, rew_list, val_list, mask_list, episode_returns\n",
    "\n",
    "def train(env, agent, config):\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    mini_batch_size = config[\"mini_batch_size\"]\n",
    "    total_steps = config[\"total_steps\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "\n",
    "    reward_history = []\n",
    "    moving_avg_rewards = deque(maxlen=10)\n",
    "    steps_collected = 0\n",
    "\n",
    "    while steps_collected < total_steps:\n",
    "        (obs_list, act_list, old_logp_list, logits_list,\n",
    "         rew_list, val_list, mask_list, ep_returns) = collect_batch(env, agent.policy, agent.value_fn, batch_size)\n",
    "        steps_collected += len(rew_list)\n",
    "\n",
    "        values = [v.item() for v in val_list]\n",
    "        advantages = compute_gae(rew_list, values, mask_list, config[\"gamma\"], config[\"gae_lambda\"])\n",
    "        returns = [a + v for a, v in zip(advantages, values)]\n",
    "\n",
    "        obs_batch = torch.stack(obs_list)\n",
    "        act_batch = torch.stack(act_list)\n",
    "        old_logp_batch = torch.stack(old_logp_list)\n",
    "        old_logits_batch = torch.stack(logits_list)\n",
    "        adv_batch = torch.tensor(advantages, dtype=torch.float32)\n",
    "        ret_batch = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "        adv_batch = (adv_batch - adv_batch.mean()) / (adv_batch.std() + 1e-8)\n",
    "\n",
    "        idxs = np.arange(batch_size)\n",
    "        for _ in range(epochs):\n",
    "            np.random.shuffle(idxs)\n",
    "            for start in range(0, batch_size, mini_batch_size):\n",
    "                end = start + mini_batch_size\n",
    "                mb_idx = idxs[start:end]\n",
    "\n",
    "                mb_obs = obs_batch[mb_idx]\n",
    "                mb_act = act_batch[mb_idx]\n",
    "                mb_adv = adv_batch[mb_idx]\n",
    "                mb_ret = ret_batch[mb_idx]\n",
    "                mb_old_logits = old_logits_batch[mb_idx]\n",
    "\n",
    "                loss = agent.compute_loss(mb_obs, mb_act, mb_adv, mb_ret, mb_old_logits)\n",
    "\n",
    "                agent.policy.zero_grad()\n",
    "                agent.value_fn.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        avg_reward = np.mean(ep_returns)\n",
    "        moving_avg_rewards.append(avg_reward)\n",
    "        moving_avg = np.mean(moving_avg_rewards)\n",
    "\n",
    "        print(f\"Steps {steps_collected}: Avg Reward = {avg_reward:.2f}, Moving Avg (10) = {moving_avg:.2f}\")\n",
    "        reward_history.append(moving_avg)\n",
    "\n",
    "    plt.plot(reward_history)\n",
    "    plt.title(\"Training Progress\")\n",
    "    plt.xlabel(\"Evaluation Step\")\n",
    "    plt.ylabel(\"Moving Avg Reward\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate(env, policy, num_episodes=10):\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        result = env.reset()\n",
    "        if isinstance(result, tuple):\n",
    "            obs, _ = result\n",
    "        else:\n",
    "            obs = result\n",
    "\n",
    "        done = False\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        while not done:\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
    "            logits = policy(obs_tensor)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            action = dist.sample().item()\n",
    "\n",
    "            result = env.step(action)\n",
    "            if len(result) == 5:\n",
    "                next_obs, reward, terminated, truncated, _ = result\n",
    "                done = terminated or truncated\n",
    "            else:\n",
    "                next_obs, reward, done, _ = result\n",
    "\n",
    "            episode_reward += reward\n",
    "            obs = next_obs\n",
    "\n",
    "        total_reward += episode_reward\n",
    "\n",
    "    avg_reward = total_reward / num_episodes\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T20:31:46.221879Z",
     "iopub.status.busy": "2025-04-29T20:31:46.221537Z",
     "iopub.status.idle": "2025-04-29T20:31:46.542035Z",
     "shell.execute_reply": "2025-04-29T20:31:46.541131Z",
     "shell.execute_reply.started": "2025-04-29T20:31:46.221833Z"
    },
    "id": "At0c9px9pxQb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GeneralAgent:\n",
    "    def __init__(self, policy, value_fn, config):\n",
    "        self.policy = policy\n",
    "        self.value_fn = value_fn\n",
    "        self.config = config\n",
    "        self.algo = config.get(\"algo\", \"GRPO\")  # Options: GRPO, GRPO_KL, PPO_CLIP, PPO_KL, A2C\n",
    "\n",
    "    def compute_loss(self, mb_obs, mb_act, mb_adv, mb_ret, mb_old_logits=None):\n",
    "        logits = self.policy(mb_obs)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        logp = dist.log_prob(mb_act)\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        value = self.value_fn(mb_obs)\n",
    "        policy_loss = None\n",
    "\n",
    "        if self.algo == \"GRPO\":\n",
    "            policy_loss = -(mb_adv.detach() * logp).mean()\n",
    "\n",
    "        elif self.algo == \"GRPO_KL\":\n",
    "            policy_loss = -(mb_adv.detach() * logp).mean()\n",
    "            if mb_old_logits is not None:\n",
    "                old_probs = torch.softmax(mb_old_logits, dim=-1)\n",
    "                kl = (old_probs * (torch.log(old_probs + 1e-8) - torch.log(probs + 1e-8))).sum(-1).mean()\n",
    "                policy_loss += self.config.get(\"kl_coeff\", 0.5) * kl\n",
    "\n",
    "        elif self.algo == \"PPO_CLIP\":\n",
    "            with torch.no_grad():\n",
    "                old_probs = torch.softmax(mb_old_logits, dim=-1)\n",
    "                old_dist = torch.distributions.Categorical(old_probs)\n",
    "                old_logp = old_dist.log_prob(mb_act)\n",
    "\n",
    "            ratio = torch.exp(logp - old_logp)\n",
    "            clip_eps = self.config.get(\"clip_eps\", 0.2)\n",
    "            clipped_ratio = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps)\n",
    "\n",
    "            policy_loss = -torch.min(ratio * mb_adv.detach(), clipped_ratio * mb_adv.detach()).mean()\n",
    "\n",
    "        elif self.algo == \"PPO_KL\":\n",
    "            with torch.no_grad():\n",
    "                old_probs = torch.softmax(mb_old_logits, dim=-1)\n",
    "            kl = (old_probs * (torch.log(old_probs + 1e-8) - torch.log(probs + 1e-8))).sum(-1).mean()\n",
    "\n",
    "            policy_loss = -(mb_adv.detach() * logp).mean()\n",
    "            policy_loss += self.config.get(\"kl_coeff\", 0.5) * kl\n",
    "\n",
    "        elif self.algo == \"A2C\":\n",
    "            policy_loss = -(mb_adv.detach() * logp).mean()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown algo {self.algo}\")\n",
    "\n",
    "        value_loss = ((value - mb_ret) ** 2).mean()\n",
    "        loss = policy_loss + value_loss\n",
    "\n",
    "        if self.config.get(\"entropy_bonus\", True):\n",
    "            loss -= self.config.get(\"entropy_coeff\", 0.01) * entropy\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "execution_failed": "2025-04-30T08:30:58.259Z",
     "iopub.execute_input": "2025-04-29T20:31:46.543265Z",
     "iopub.status.busy": "2025-04-29T20:31:46.543012Z"
    },
    "id": "Z4QfjISRqBRs",
    "outputId": "83579344-6ead-4a79-fbae-42e20a50ac78",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# from models import Policy, Value\n",
    "# from agent import GeneralAgent\n",
    "# from trainer import train, evaluate\n",
    "\n",
    "env_configs = {\n",
    "    \"CartPole-v1\": {\n",
    "        \"batch_size\": 8192,\n",
    "        \"mini_batch_size\": 512,\n",
    "        \"epochs\": 10,\n",
    "        \"total_steps\": 200_000,\n",
    "        \"gamma\": 0.99,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"entropy_bonus\": True,\n",
    "        \"entropy_coeff\": 0.01,\n",
    "        \"kl_coeff\": 0.5,\n",
    "        \"clip_eps\": 0.2,\n",
    "    },\n",
    "    \"LunarLander-v2\": {\n",
    "        \"batch_size\": 16384,\n",
    "        \"total_steps\": 1_000_000,\n",
    "        \"mini_batch_size\": 1024,\n",
    "        \"epochs\": 10,\n",
    "        \"gamma\": 0.99,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"entropy_bonus\": True,\n",
    "        \"entropy_coeff\": 0.02, #stronger exploration bonus\n",
    "        \"kl_penalty\": True,\n",
    "        \"kl_coeff\": 1.0,\n",
    "        \"clip_eps\": 0.2,\n",
    "    },\n",
    "    \"MountainCar-v0\": {\n",
    "      \"batch_size\": 8192,\n",
    "      \"mini_batch_size\": 512,\n",
    "      \"epochs\": 10,\n",
    "      \"total_steps\": 500_000,\n",
    "      \"gamma\": 0.99,\n",
    "      \"gae_lambda\": 0.95,\n",
    "      \"entropy_bonus\": True,\n",
    "      \"entropy_coeff\": 0.02,\n",
    "      \"kl_coeff\": 0.5,\n",
    "      \"clip_eps\": 0.2,\n",
    "  },\n",
    "#     \"PongNoFrameskip-v4\":{\n",
    "#       \"batch_size\": 8192,\n",
    "#       \"mini_batch_size\": 512,\n",
    "#       \"epochs\": 4,\n",
    "#       \"total_steps\": 5_000_000,\n",
    "#       \"gamma\": 0.99,\n",
    "#       \"gae_lambda\": 0.95,\n",
    "#       \"entropy_bonus\": True,\n",
    "#       \"entropy_coeff\": 0.01,\n",
    "#       \"kl_coeff\": 0.2,\n",
    "#       \"clip_eps\": 0.1,\n",
    "#   },\n",
    "  \n",
    "}\n",
    "\n",
    "algorithms = [\"GRPO\", \"GRPO_KL\", \"PPO_CLIP\", \"PPO_KL\", \"A2C\"]\n",
    "\n",
    "\n",
    "final_results = []\n",
    "\n",
    "for env_id, base_config in env_configs.items():\n",
    "    for algo in algorithms:\n",
    "        print(f\"\\nRunning {algo} on {env_id}:\")\n",
    "\n",
    "\n",
    "        config = base_config.copy()\n",
    "        config[\"algo\"] = algo\n",
    "        config[\"kl_penalty\"] = algo in [\"GRPO_KL\", \"PPO_KL\"]\n",
    "\n",
    "        if \"Pong\" in env_id:\n",
    "            #env = gym.make(env_id, frameskip=1, repeat_action_probability=0.0)\n",
    "            env = gym.make(env_id)\n",
    "            env = gym.wrappers.AtariPreprocessing(env, grayscale_obs=True, scale_obs=False, frame_skip=1, noop_max=30)\n",
    "            env = gym.wrappers.FrameStack(env, 4)\n",
    "            act_dim = env.action_space.n\n",
    "            policy = CNNPolicy(act_dim)\n",
    "            print(policy)\n",
    "            value_fn = CNNValue()\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "            obs_dim = env.observation_space.shape[0]\n",
    "            act_dim = env.action_space.n\n",
    "            policy = Policy(obs_dim, act_dim)\n",
    "            value_fn = Value(obs_dim)\n",
    "\n",
    "        optimizer = optim.Adam(list(policy.parameters()) + list(value_fn.parameters()), lr=3e-4)\n",
    "        agent = GeneralAgent(policy, value_fn, config)\n",
    "\n",
    "        reward_curve = train(env, agent, config)\n",
    "\n",
    "        final_reward = evaluate(env, agent.policy, num_episodes=10)\n",
    "\n",
    "        final_results.append({\n",
    "            \"Environment\": env_id,\n",
    "            \"Algorithm\": algo,\n",
    "            \"FinalReward\": final_reward\n",
    "        })\n",
    "\n",
    "        print(f\"{algo} on {env_id} Final Reward: {final_reward:.2f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
